{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631aef4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/marcos/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# load all metadata\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1469b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los datos\n",
    "data = pd.read_csv('justice.csv')\n",
    "X = data.dropna()\n",
    "Y = X.pop('first_party_winner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f618b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasa las etiquetas escritas a valores numéricos\n",
    "def getLabels(Y):\n",
    "    names = []\n",
    "    newY =  []\n",
    "    for i in Y:\n",
    "        if i not in names:\n",
    "            names.append(i)\n",
    "        newY.append(names.index(i))\n",
    "    return newY, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "974ada63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pasamos la etiquetas de FirstPartyWinner a valores numéricos\n",
    "y_labels, y_names = getLabels(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64ce4ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimina signos de puntuación y todo a minúsculas\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3161ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las stop words\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ba16ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a74c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizamos\n",
    "def lemmatization(texts):\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        lemText = []\n",
    "        for sent in text:\n",
    "            stemmed = ps.stem(sent)\n",
    "            lemText.append(lem.lemmatize(stemmed))\n",
    "        texts_out.append(lemText)\n",
    "    return texts_out\n",
    "\n",
    "data_lemmatized = lemmatization(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a067110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def train_test(test):\n",
    "    return train_test_split(X,Y,test_size=test, random_state=1)\n",
    "#Probamos con 30% de test y 70% de train\n",
    "X_train, X_test, Y_train, Y_test = train_test(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e8c64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##KNN Y TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "924c2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def decision_tree(md):\n",
    "    clf = DecisionTreeClassifier(max_depth=md)\n",
    "    clf = clf.fit(X_train,Y_train)\n",
    "    \n",
    "    Y_pred = clf.predict(X_test)\n",
    "    probas = clf.predict_proba(X_test)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(Y_test,Y_pred))\n",
    "    return clf\n",
    "#Probamos con máxima repetición 3\n",
    "#clf = decision_tree(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13762b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZAMOS EL ÁRBOL\n",
    "from six import StringIO \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "def visualizar_tree():\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "    graph.write_png('diabetes.png')\n",
    "    Image(graph.create_png())\n",
    "#visualizar_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e3be114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "def knn(k,K,d,D):\n",
    "    indice = 1\n",
    "    F1 = []\n",
    "    total = []\n",
    "    weigths = ['uniform','distance']\n",
    "    for k_value in range(int(k),int(K) + 1, 2):\n",
    "        for d_value in range(int(d),int(D) + 1):\n",
    "            for weigth in weigths:\n",
    "                clf = KNeighborsClassifier(n_neighbors=k_value,\n",
    "                                           weights=weight,\n",
    "                                           algorithm='auto',\n",
    "                                           leaf_size=30,\n",
    "                                           p=d_value)\n",
    "                clf.class_weigth = \"balanced\"\n",
    "                clf.fit(X_train,Y_train)\n",
    "                \n",
    "                Y_pred = clf.predict(X_test)\n",
    "                probas = clf.predict_proba(X_test)\n",
    "                \n",
    "                info = [\n",
    "                    \"Caso \" + str(indice) + \" : \" + \"k :\" + str(k_value) + \", \" + \"d : \" + str(\n",
    "                        d_value) + \", \" + \"weight : \" + str(weight) + \" ---> \",\n",
    "                    f1_score(Y_test, Y_pred, average=None),\n",
    "                    classification_report(Y_test, Y_pred),\n",
    "                    confusion_matrix(Y_test, Y_pred, y_names)\n",
    "                ]\n",
    "                indice += 1\n",
    "                \n",
    "                F1.append(f1_score(y_test, predictions, average=None))\n",
    "                total.append([info[0], f1_score(y_test, predictions, average=None), info[2]])\n",
    "                file = open('knn.csv', 'a')\n",
    "                file.write(\"\\n\" + str(info[0]) + str(info[1]) + \"\\n\" + str(info[2]) + \"\\n\")\n",
    "    F1_max = -1\n",
    "    for maximo in F1:\n",
    "        if maximo[0] > F1_max:\n",
    "            F1_max = maximo[0]\n",
    "\n",
    "    for infor in total:\n",
    "        f1 = infor[1]\n",
    "        if f1[0] == F1_max:\n",
    "            file = open('mejor_knn.csv', 'a')\n",
    "            file.write(\"\\n\" + str(infor[0]) + str(f1[0]) + \"\\n\" + str(infor[2]) + \"\\n\")\n",
    "##Probamos con \n",
    "#knn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NO HE SEGUIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crea un modelo con n numero de Topics en relación a los datos administrados\n",
    "#Utilizamos 202 porque hemos visto en el proyecto que son los que mejor Accuracy nos proporcionan\n",
    "def get_topicModeling(data, n_topics):\n",
    "    #Topic modeling\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    texts = data\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=n_topics, random_state=100, passes=10, eval_every=None)\n",
    "    count = 0\n",
    "    topics=[]\n",
    "    for i in lda_model.print_topics():\n",
    "        topics.append(i)\n",
    "        count += 1\n",
    "    return lda_model,corpus,topics\n",
    "\n",
    "#model, corpus, topics = get_topicModeling(data_lemmatized,202)\n",
    "##NO SE SI HACE FALTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d379e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorized(model, corpus):\n",
    "    n_topics = model.num_topics\n",
    "    vectorized = []\n",
    "    #Recorrer los documentos\n",
    "    for i in model[corpus]:\n",
    "        #Inicializar index como lista de ceros, de longitud 'n_topics'\n",
    "        index = [0]*n_topics\n",
    "        #Recorrer cada palabra de cada documento\n",
    "        for n in i:\n",
    "            #En index[8] guardamos la informacion del tópico 8\n",
    "            index[n[0]] = n[1]\n",
    "        vectorized.append(index)\n",
    "    vectorized = pd.DataFrame(vectorized)\n",
    "    #Filas documentos (facts)\n",
    "    #Columnas topics\n",
    "    return vectorized\n",
    "\n",
    "\n",
    "vectorized = get_vectorized(model,corpus)\n",
    "print(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45baf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparar dimensiones antes y después de hacer el pca\n",
    "from sklearn.decomposition import  PCA\n",
    "\n",
    "#Redimensiona los datos introducidos a las dimensiones pedidas\n",
    "#Utilizamos 2 para el número de PCA porque hemos visto en el proyecto que obtenemos gran silueta\n",
    "def get_PCA(data, nPCA):\n",
    "    print(\"Before:\", data.shape)\n",
    "    #pca = PCA(n_components='mle', svd_solver='full')\n",
    "    pca = PCA(n_components=nPCA)\n",
    "    pca.fit(data)\n",
    "    data_PCA= pca.transform(data)\n",
    "    print(\"After:\", data_PCA.shape)\n",
    "    return data_PCA, pca\n",
    "\n",
    "data_PCA, pca = get_PCA(vectorized,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devuelve la precision a partir de una matriz de confusión\n",
    "def get_accuracy_unorder(cm):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    row = 0\n",
    "    for i in cm:\n",
    "        column = 0\n",
    "        for n in i:\n",
    "            total += n\n",
    "            if column == row and column < len(cm[0]):\n",
    "                correct += n\n",
    "            column += 1\n",
    "        row += 1\n",
    "    return total, correct , correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#Devuelve la matriz de confusion\n",
    "def get_cm(Y_labels, predictions, names, visual = True):\n",
    "    # El atributo generado por K-means es int, hay que pasarlos a string\n",
    "    to_string = lambda x : str(x)\n",
    "    # Obtener matriz de confusión Class to clustering eval\n",
    "    cm = confusion_matrix(predictions, Y_labels)\n",
    "    #Borrar la columnas vacias\n",
    "    columns = cm.any(axis=0)\n",
    "    delete = []\n",
    "    for i in range(len(columns)):\n",
    "        if not columns[i]:\n",
    "            delete.append(i)\n",
    "    cm = np.delete(cm, delete, axis=1)\n",
    "\n",
    "\n",
    "    ylabels=[None for i in range(len(cm))]\n",
    "    ymax=[0 for i in range(len(cm))]\n",
    "    cm_copy = cm.copy()\n",
    "    for i in range(19):\n",
    "        cont=0\n",
    "        for i in cm_copy:\n",
    "            if ylabels[cont] == None:\n",
    "                if i.argmax() in ylabels:\n",
    "                    x=ymax[i.argmax()]\n",
    "                    y=i.max()\n",
    "                    if y > x[1] :\n",
    "                        ylabels[x[0]] = None\n",
    "                        ylabels[cont] = i.argmax()\n",
    "                        ymax[i.argmax()]=(cont,i.max())\n",
    "                        cm_copy[x[0]][i.argmax()]=0\n",
    "                    else:\n",
    "                        cm_copy[cont][i.argmax()]=0\n",
    "                else:\n",
    "                    ylabels[cont] = i.argmax()\n",
    "                    ymax[i.argmax()]=(cont,i.max())\n",
    "            cont=cont+1\n",
    "\n",
    "    cm_new = [[] for i in range(len(cm))]\n",
    "    cont = 0\n",
    "    used = [i for i in range(len(cm))]\n",
    "    cont_down = 0\n",
    "    for i in ylabels:\n",
    "        if i is not None:\n",
    "            cm_new[i] = cm[cont].tolist()\n",
    "            used.remove(cont)\n",
    "        elif cont_down < len(cm)- len(cm[0]):\n",
    "            cm_new[len(cm)-cont_down - 1] = cm[cont].tolist()\n",
    "            used.remove(cont)\n",
    "            cont_down +=1\n",
    "        cont += 1\n",
    "\n",
    "    for i in range(len(cm_new)):\n",
    "        if not cm_new[i]:\n",
    "            cm_new[i] = cm[used[0]]\n",
    "            ylabels[used[0]] = i\n",
    "            used.remove(used[0])\n",
    "\n",
    "    new_ylabels = [None] * len(ylabels)\n",
    "    for i in range(len(ylabels)):\n",
    "        if ylabels[i] is not None:\n",
    "            new_ylabels[ylabels[i]] = names[ylabels[i]]\n",
    "    ylabels = new_ylabels\n",
    "\n",
    "    if visual:\n",
    "        # Mapa de calor a partir de la matriz de confusion\n",
    "        ax = sns.heatmap(cm_new, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=names, yticklabels=ylabels)\n",
    "        print(get_accuracy_unorder(cm_new))\n",
    "\n",
    "    return cm_new\n",
    "\n",
    "cm = get_cm(y1_labels, k_labels, y1_names, True)\n",
    "#El valor real es el de abajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def getAll(range_topics, range_PCA, range_clusters, data):\n",
    "    scores = []\n",
    "    for t in range_topics:\n",
    "        model,corpus,topics = get_topicModeling(data, t)\n",
    "        vectorized = get_vectorized(model,corpus)\n",
    "        for p in [x for x in range_PCA if x <= t]:\n",
    "            data_PCA,pca = get_PCA(vectorized,p)\n",
    "            list_of_clusters,tree = hierarchical_clustering(data_PCA)\n",
    "            for c in range_clusters:\n",
    "                k_labels,dist,k_clusters = get_nClusters(list_of_clusters,c)\n",
    "                sj = silhouette_score(data_PCA,k_labels)\n",
    "                cm = get_cm(y1_labels, k_labels, y1_names, False)\n",
    "                acc1 = get_accuracy_unorder(cm)[2]\n",
    "                cm = get_cm(y2_labels, k_labels, y2_names, False)\n",
    "                acc2 = get_accuracy_unorder(cm)[2]\n",
    "                scores.append([c,dist,p,t,sj,acc1,acc2])\n",
    "                df_scores = pd.DataFrame(scores)\n",
    "    df_scores = df_scores.set_axis([\"nClusters\", \"dist\", \"PCA\", \"nTopics\", \"silhouetteValue\", \"Acc_FirstPartyWinner\", \"Acc_IssueArea\"], axis=\"columns\")\n",
    "    df_scores.to_csv(\"scores3.csv\", encoding=\"utf-8\", sep=',',index=False, header=True)\n",
    "    return scores\n",
    "\n",
    "range_topics = [i for i in range(200,401,50)]\n",
    "#scores = getAll(range_topics, range(2,403,100), range(2,20,1), data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_scores(csv, x_label, y_label, func):\n",
    "    df_scores = pd.read_csv(csv)\n",
    "    x_index = list(df_scores.columns.values).index(x_label)\n",
    "    y_index = list(df_scores.columns.values).index(y_label)\n",
    "    nTopics = []\n",
    "    nTopicsScore = []\n",
    "    for d in df_scores.values:\n",
    "        if d[x_index] in nTopics:\n",
    "            nTopicsScore[nTopics.index(d[x_index])].append(d[y_index])\n",
    "        else:\n",
    "            nTopics.append(d[x_index])\n",
    "            nTopicsScore.append([d[y_index]])\n",
    "\n",
    "    for s in range(len(nTopicsScore)):\n",
    "        nTopicsScore[s] = func(nTopicsScore[s])\n",
    "    df = pd.DataFrame({\n",
    "          'x_axis': nTopics,\n",
    "          'y_axis': nTopicsScore\n",
    "      })\n",
    "    df.sort_values('x_axis', ignore_index=True, inplace=True)\n",
    "    plt.plot('x_axis', 'y_axis', data=df, linestyle='-', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos del número de Topicos, PCA, y número de Clusters, en relación con la precisión en FirstPartyWinner\n",
    "get_plot_scores(\"scores1.csv\",\"nTopics\",\"Acc_FirstPartyWinner\", np.mean)\n",
    "get_plot_scores(\"scores1.csv\", \"PCA\", \"Acc_FirstPartyWinner\", np.mean)\n",
    "get_plot_scores(\"scores1.csv\",\"nClusters\",\"Acc_FirstPartyWinner\", np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e4154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficos del número de Topicos, PCA, y número de Clusters, en relación con el valor de la silhouette\n",
    "get_plot_scores(\"scores1.csv\",\"nTopics\",\"silhouetteValue\", np.mean)\n",
    "get_plot_scores(\"scores1.csv\", \"PCA\", \"silhouetteValue\", np.mean)\n",
    "get_plot_scores(\"scores1.csv\",\"nClusters\",\"silhouetteValue\", np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "\n",
    "def save_model(data, nTopics, nPCA, nClusters, trainLabels, trainNames, csvName):\n",
    "    model,corpus, topics = get_topicModeling(data,nTopics)\n",
    "    pk.dump(model, open(\"lda_\" + csvName + \".pkl\",\"wb\"))\n",
    "    vectorized = get_vectorized(model,corpus)\n",
    "    data_PCA, pca = get_PCA(vectorized,nPCA)\n",
    "\n",
    "    pk.dump(pca, open(\"pca_\" + csvName + \".pkl\",\"wb\"))\n",
    "    list_of_clusters, tree = hierarchical_clustering(data_PCA)\n",
    "    labels, dist, clusters = get_nClusters(list_of_clusters, nClusters)\n",
    "    cm = get_cm(trainLabels, labels, trainNames)\n",
    "    get_silhouette(data_PCA,list_of_clusters,[nClusters])\n",
    "    plot_dendrogram(tree,labels,dist)\n",
    "\n",
    "    pd.DataFrame(data_PCA).to_csv(\"data_\" + csvName + \".csv\", encoding=\"utf-8\", sep=',', index = False, header=True)\n",
    "    df_clusters = pd.DataFrame(list_of_clusters)\n",
    "    df_clusters = df_clusters.set_axis([\"nIt\", \"dist\", \"Clusters\"], axis=\"columns\")\n",
    "    df_clusters.to_csv(csvName + \".csv\", encoding=\"utf-8\", sep=',', index = False, header=True)\n",
    "    df_tree = pd.DataFrame(tree[0])\n",
    "    df_tree.insert(2, \"dist\", tree[1])\n",
    "    df_tree = df_tree.set_axis([\"tree1\", \"tree2\", \"dist\"], axis=\"columns\")\n",
    "    df_tree.to_csv(\"tree_\" + csvName + \".csv\", encoding=\"utf-8\", sep=',', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac63e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "def randomLabels(data, n_clusters):\n",
    "    labels = []\n",
    "    clusters = []\n",
    "    for x in range(n_clusters):\n",
    "        clusters.append([])\n",
    "    for i in range(len(data)):\n",
    "        rand_n = randrange(n_clusters)\n",
    "        labels.append(rand_n)\n",
    "        clusters[rand_n].append(i)\n",
    "    return labels, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5392f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = get_cm(y1_labels,randomLabels(data_lemmatized,2)[0],y1_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a00fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(data_lemmatized,202,2,2,y1_labels,y1_names,\"clustersFirstPartyWinner_202_2_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def cargar_list_of_clusters(file_name):\n",
    "    list_of_clusters = pd.read_csv(file_name + \".csv\").values\n",
    "    df_tree = pd.read_csv(\"tree_\" + file_name + \".csv\")\n",
    "    data = pd.read_csv(\"data_\" + file_name + \".csv\").values\n",
    "    tree = [0] * 2\n",
    "    tree[0] = df_tree.get([\"tree1\",\"tree2\"]).values\n",
    "    tree[1] = df_tree[\"dist\"].values\n",
    "    for i in range(len(list_of_clusters)):\n",
    "        list_of_clusters[i][2] = ast.literal_eval(list_of_clusters[i][2])\n",
    "    model = pk.load(open(\"lda_\" + file_name + \".pkl\",'rb'))\n",
    "    pca = pk.load(open(\"pca_\" + file_name + \".pkl\",'rb'))\n",
    "\n",
    "\n",
    "    return list_of_clusters, tree, model, pca, data\n",
    "list_of_clusters, tree, model, pca, data = cargar_list_of_clusters(\"clustersIssueArea_12_2_3\")\n",
    "k_labels, dist, k_clusters = get_nClusters(list_of_clusters,3)\n",
    "plot_dendrogram(tree, k_labels, dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
